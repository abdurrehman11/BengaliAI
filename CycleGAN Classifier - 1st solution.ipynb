{"cells":[{"metadata":{},"cell_type":"markdown","source":"- **Auto Augmentation** - https://towardsdatascience.com/how-to-improve-your-image-classifier-with-googles-autoaugment-77643f0be0c9\n- **Synthesize Font Images** - https://www.kaggle.com/c/bengaliai-cv19/discussion/127938#775496\n- **BengaliAI First Solution Writeup** - https://www.kaggle.com/c/bengaliai-cv19/discussion/135984\n- **Bengali External Datasets** - https://www.kaggle.com/c/bengaliai-cv19/discussion/122396\n- **Layer Normalization** - https://mlexplained.com/2018/01/13/weight-normalization-and-layer-normalization-explained-normalization-in-deep-learning-part-2/"},{"metadata":{},"cell_type":"markdown","source":"# Install libraqm"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!apt-get install -y libfreetype6-dev libharfbuzz-dev libfribidi-dev gtk-doc-tools\n!wget -O raqm-0.7.0.tar.gz https://raw.githubusercontent.com/python-pillow/pillow-depends/master/raqm-0.7.0.tar.gz\n!tar -xzvf /kaggle/working/raqm-0.7.0.tar.gz\n!cd /kaggle/working/raqm-0.7.0/ &amp;&amp; ./configure --prefix=/usr &amp;&amp; make -j4 &amp;&amp; make -j4 install","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Synthesize Font Images"},{"metadata":{"trusted":true},"cell_type":"code","source":"import PIL.Image as Image, PIL.ImageDraw as ImageDraw, PIL.ImageFont as ImageFont\n\nHEIGHT = 137\nWIDTH = 236\n\ndef image_from_char(font_url, grapheme_char, grapheme_size):\n    image = Image.new('RGB', (WIDTH, HEIGHT))\n    draw = ImageDraw.Draw(image)\n    myfont = ImageFont.truetype(font_url, grapheme_size)\n    w, h = draw.textsize(grapheme_char, font=myfont)\n    draw.text(((WIDTH - w) / 2,(HEIGHT - h) / 3), grapheme_char, font=myfont)\n\n    return image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"font = pd.read_csv(\"../input/bengaliai-cv19-font/font.csv\", index_col=0)\nfont.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grapheme_sizes = [84, 96, 108, 120]\n\nfont_1397 = font.iloc[1397]['grapheme']\nkalpurush_fonts = '/kaggle/input/kalpurush-fonts/kalpurush-2.ttf'\nnikoshlightban_fonts = '/kaggle/input/nikoshlightbanfonts/NikoshLightBan.ttf'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"font_1397_image = image_from_char(nikoshlightban_fonts, font_1397, grapheme_sizes[0])\nfont_1397_image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"font_1397_image = image_from_char(nikoshlightban_fonts, font_1397, grapheme_sizes[1])\nfont_1397_image","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"font_1397_image = image_from_char(nikoshlightban_fonts, font_1397, grapheme_sizes[2])\nfont_1397_image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"font_1397_image = image_from_char(nikoshlightban_fonts, font_1397, grapheme_sizes[3])\nfont_1397_image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = np.asarray(font_1397_image, dtype=\"int32\")\n# data = data.flatten()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Font Data (bengaliai-cv19-font)\n- **Font Files** were generated using `.ttf(true type font)` files which were publically available fonts for Bengali. `ttf files` files used in this competition are `kalpurush-2.ttf` and `NikoshLightBan.ttf`.\n\n- **font.csv** file contains `(168*11*8) = 14784` unique graphemes and total of `14784 * 4(four diff. size graphemes) = 59136` graphemes and their respecitve labels\n\n- **font_image_data_0.parquet** --> 14784 graphemes (size 80)\n- **font_image_data_0.parquet** --> 14784 graphemes (size 96)\n- **font_image_data_0.parquet** --> 14784 graphemes (size 108)\n- **font_image_data_0.parquet** --> 14784 graphemes (size 120)"},{"metadata":{},"cell_type":"markdown","source":"# Script for Predicted relationship b/w grapheme and labels"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\nclass_map = pd.read_csv('../input/bengaliai-cv19/class_map.csv')\n\ngrapheme_root = class_map[class_map['component_type'] == 'grapheme_root']\nvowel_diacritic = class_map[class_map['component_type'] == 'vowel_diacritic']\nconsonant_diacritic = class_map[class_map['component_type'] == 'consonant_diacritic']\n\ngrapheme_root_list = grapheme_root['component'].tolist()\nvowel_diacritic_list = vowel_diacritic['component'].tolist()\nconsonant_diacritic_list = consonant_diacritic['component'].tolist()\n\ndef label_to_grapheme(grapheme_root, vowel_diacritic, consonant_diacritic):\n    if consonant_diacritic == 0:\n        if vowel_diacritic == 0:\n            return grapheme_root_list[grapheme_root]\n        else:\n            return grapheme_root_list[grapheme_root] + vowel_diacritic_list[vowel_diacritic]\n        \n    elif consonant_diacritic == 1:\n        if vowel_diacritic == 0:\n            return grapheme_root_list[grapheme_root] + consonant_diacritic_list[consonant_diacritic]\n        else:\n            return grapheme_root_list[grapheme_root] + vowel_diacritic_list[vowel_diacritic] + \\\n                   consonant_diacritic_list[consonant_diacritic]\n        \n    elif consonant_diacritic == 2:\n        if vowel_diacritic == 0:\n            return consonant_diacritic_list[consonant_diacritic] + grapheme_root_list[grapheme_root]\n        else:\n            return consonant_diacritic_list[consonant_diacritic] + grapheme_root_list[grapheme_root] + \\\n                   vowel_diacritic_list[vowel_diacritic]\n        \n    elif consonant_diacritic == 3:\n        if vowel_diacritic == 0:\n            return consonant_diacritic_list[consonant_diacritic][:2] + grapheme_root_list[grapheme_root] + \\\n                   consonant_diacritic_list[consonant_diacritic][1:]\n        else:\n            return consonant_diacritic_list[consonant_diacritic][:2] + grapheme_root_list[grapheme_root] + \\\n                   consonant_diacritic_list[consonant_diacritic][1:] + vowel_diacritic_list[vowel_diacritic]\n        \n    elif consonant_diacritic == 4:\n        if vowel_diacritic == 0:\n            return grapheme_root_list[grapheme_root] + consonant_diacritic_list[consonant_diacritic]\n        else:\n            if grapheme_root == 123 and vowel_diacritic == 1:\n                return grapheme_root_list[grapheme_root] + '\\u200d' + consonant_diacritic_list[consonant_diacritic] + \\\n                       vowel_diacritic_list[vowel_diacritic]\n            return grapheme_root_list[grapheme_root]  + consonant_diacritic_list[consonant_diacritic] + \\\n                   vowel_diacritic_list[vowel_diacritic]\n        \n    elif consonant_diacritic == 5:\n        if vowel_diacritic == 0:\n            return grapheme_root_list[grapheme_root] + consonant_diacritic_list[consonant_diacritic]\n        else:\n            return grapheme_root_list[grapheme_root] + consonant_diacritic_list[consonant_diacritic] + \\\n                   vowel_diacritic_list[vowel_diacritic]\n        \n    elif consonant_diacritic == 6:\n        if vowel_diacritic == 0:\n            return grapheme_root_list[grapheme_root] + consonant_diacritic_list[consonant_diacritic]\n        else:\n            return grapheme_root_list[grapheme_root] + consonant_diacritic_list[consonant_diacritic] + \\\n                   vowel_diacritic_list[vowel_diacritic]\n        \n    elif consonant_diacritic == 7:\n        if vowel_diacritic == 0:\n            return consonant_diacritic_list[2] + grapheme_root_list[grapheme_root] + consonant_diacritic_list[2][::-1]\n        else:\n            return consonant_diacritic_list[2] + grapheme_root_list[grapheme_root] + consonant_diacritic_list[2][::-1] + \\\n                   vowel_diacritic_list[vowel_diacritic]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_to_grapheme(67, 6, 3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Important imports and installation"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install efficientnet-pytorch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch import nn\nimport torchvision\nfrom torchvision import transforms\nimport albumentations as A\nfrom efficientnet_pytorch import EfficientNet\nimport gc\nimport cv2\nfrom tqdm.notebook import tqdm\nimport sklearn.metrics\nimport json","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MEAN = [0.5, 0.5, 0.5]\nSTD = [0.5, 0.5, 0.5]\nIMG_HEIGHT = 224\nIMG_WIDTH = 224\nBATCH_SIZE = 32\nEPOCH = 40\nTQDM_DISABLE = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_images(paths):\n    all_images = []\n    \n    for path in paths:\n        image_df = pd.read_parquet(path)\n        images = image_df.iloc[:, 1:].values.reshape(-1, 137, 236).astype(np.uint8)\n        \n        del image_df\n        gc.collect()\n        \n        all_images.append(images)\n    all_images = np.concatenate(all_images)\n    \n    return all_images","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"font_data = pd.read_csv('../input/bengaliai-cv19-font/font.csv', index_col=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"font_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nfont_images = load_images([\n    '../input/bengaliai-cv19-font/font_image_data_0.parquet',\n    '../input/bengaliai-cv19-font/font_image_data_1.parquet',\n    '../input/bengaliai-cv19-font/font_image_data_2.parquet',\n    '../input/bengaliai-cv19-font/font_image_data_3.parquet',\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"class GraphemeDataset(torch.utils.data.Dataset):\n    \n    def __init__(self, data, images, transform=None,\n                 num_grapheme_root=168,\n                 num_vowel_diacritic=11,\n                 num_consonant_diacritic=8):\n        self.data = data\n        self.images = images\n        self.transform = transform\n        self.num_grapheme_root = num_grapheme_root\n        self.num_vowel_diacritic = num_vowel_diacritic\n        self.num_consonant_diacritic = num_consonant_diacritic\n        \n        self.grapheme_root_list = np.array(data['grapheme_root'].tolist(), \n                                           dtype=np.int64)\n        self.vowel_diacritic_list = np.array(data['vowel_diacritic'].tolist(), \n                                             dtype=np.int64)\n        self.consonant_diacritic_list = np.array(data['consonant_diacritic'].tolist(), \n                                                 dtype=np.int64)\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        grapheme_root = self.grapheme_root_list[idx]\n        vowel_diacritic = self.vowel_diacritic_list[idx]\n        consonant_diacritic = self.consonant_diacritic_list[idx]\n        \n        label = (grapheme_root * self.num_vowel_diacritic + vowel_diacritic) * self.num_consonant_diacritic + consonant_diacritic\n        \n        np_image = self.images[idx].copy()\n        out_image = self.transform(np_image)\n        return out_image, label","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Image Augmentations"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Albumentations:\n    \n    def __init__(self, augmentations):\n        self.augmentations = A.Compose(augmentations)\n        \n    def __call__(self, image):\n        image = self.augmentations(image=image)['image']\n        return image\n    \n\npreprocess = [\n    A.CenterCrop(height=137, width=IMG_WIDTH),\n    A.Resize(height=IMG_HEIGHT, width=IMG_WIDTH, always_apply=True),\n]\n\naugmentations = [\n    A.PadIfNeeded(min_height=256, min_width=256, border_mode=cv2.BORDER_CONSTANT, \n                  value=[255, 255, 255], always_apply=True),\n    A.imgaug.transforms.IAAAffine(shear=20, mode='constant', cval=255, always_apply=True),\n    A.ShiftScaleRotate(rotate_limit=20, border_mode=cv2.BORDER_CONSTANT, \n                       value=[255, 255, 255], mask_value=[255, 255, 255], always_apply=True),\n    A.RandomCrop(height=IMG_HEIGHT, width=IMG_WIDTH, always_apply=True),\n    A.Cutout(num_holes=1, max_h_size=112, max_w_size=112, fill_value=128, always_apply=True),\n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PreProcessing Transformer"},{"metadata":{},"cell_type":"markdown","source":"- https://pytorch.org/docs/0.2.0/_modules/torchvision/transforms.html#ToTensor\n\n**ToTensor()** --> Converts a `PIL.Image` or `numpy.ndarray (H x W x C)` in the range [0, 255] to \na torch.FloatTensor of shape `(C x H x W)` in the range [0.0, 1.0]."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transform = transforms.Compose([\n    np.uint8,\n    transforms.Lambda(lambda x: np.array([x, x, x]).transpose((1, 2, 0)) ),\n    np.uint8,\n    Albumentations(preprocess + augmentations),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=MEAN, std=STD),\n#     transforms.ToPILImage(),\n])\n\nvalid_transform = transforms.Compose([\n    np.uint8,\n    transforms.Lambda(lambda x: np.array([x, x, x]).transpose((1, 2, 0)) ),\n    np.uint8,\n    Albumentations(preprocess),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=MEAN, std=STD),\n#     transforms.ToPILImage(),\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"font_dataset = GraphemeDataset(font_data, font_images, train_transform)\nvalid_dataset = GraphemeDataset(font_data, font_images, valid_transform)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class BengaliModel(nn.Module):\n    \n    def __init__(self, backbone, hidden_size=2560, class_num=168*11*7):\n        super(BengaliModel, self).__init__()\n        self.backbone = backbone\n        self._avg_pooling = nn.AdaptiveAvgPool2d(1)\n        self.ln = nn.LayerNorm(hidden_size)\n        self.fc = nn.Linear(hidden_size, class_num)\n        \n    def forward(self, inputs):\n        bs = inputs.shape[0]\n        feature = self.backbone.extract_features(inputs)\n        feature_vector = self._avg_pooling(feature)\n        feature_vector = feature_vector.view(bs, -1)\n        feature_vector = self.ln(feature_vector)\n        out = self.fc(feature_vector)\n        \n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"backbone = EfficientNet.from_name('efficientnet-b0')\nclassifier = BengaliModel(backbone, hidden_size=1280, class_num=168*11*8).to(device)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create Data Loader"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_train_samples = int(len(font_dataset)) * (EPOCH)\nnum_valid_samples = int(len(valid_dataset)) * (EPOCH)\n\nfont_sampler = torch.utils.data.RandomSampler(font_dataset, replacement=True, num_samples=num_train_samples)\nvalid_sampler = torch.utils.data.RandomSampler(valid_dataset, replacement=True, num_samples=num_valid_samples)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"font_loader = torch.utils.data.DataLoader(\n    font_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=1,\n    pin_memory=True,\n    drop_last=True,\n    sampler=font_sampler\n)\n\nvalid_loader = torch.utils.data.DataLoader(\n    valid_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=1,\n    pin_memory=True,\n    drop_last=True,\n    sampler=valid_sampler\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"font_loader_iter = iter(font_loader)\nvalid_loader_iter = iter(valid_loader)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_step(model, train_iter, criterion, optimizer, schedular, device):\n    image, label = next(train_iter)\n    image = image.to(device)\n    label = label.to(device)\n    \n    optimizer.zero_grad()\n    out = model(image)\n\n    loss = criterion(out, label)\n    loss.backward()\n    optimizer.step()\n    schedular.step()\n    \n    return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = torch.optim.AdamW(classifier.parameters())\nclassifier_loss = nn.CrossEntropyLoss()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_steps_per_epoch = len(font_loader) // EPOCH\nnum_valid_steps_per_epoch = len(valid_dataset) // EPOCH\ntrain_steps = num_steps_per_epoch * EPOCH\nWARM_UP_STEP = train_steps * 0.5\n\ndef warm_up_linear_decay(step):\n    if step < WARM_UP_STEP:\n        return 1.0\n    else:\n        print(step, train_steps, WARM_UP_STEP)\n        return (train_steps - step) / (train_steps - WARM_UP_STEP)\n    \nschedular = torch.optim.lr_scheduler.LambdaLR(optimizer, warm_up_linear_decay)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log = []\nbest_score = 0.\n\nfor epoch in range(EPOCH):\n    classifier.train()\n    metric = {}\n    losses = []\n    for i in tqdm(range(num_steps_per_epoch), disable=TQDM_DISABLE):\n        loss = train_step(\n            classifier, \n            font_loader_iter, \n            classifier_loss, \n            optimizer, \n            schedular,\n            device\n        )\n        losses.append(loss.item())\n        \n    metric['train/loss'] = sum(losses) / len(losses)\n    metric['epoch'] = epoch\n    print(f\"============== Train loss on {epoch}: {metric['train/loss']} ==============\")\n    \n    classifier.eval()\n    preds = []\n    labels = []\n    \n    for i in tqdm(range(num_valid_steps_per_epoch), disable=TQDM_DISABLE):\n        image, label = next(valid_loader_iter)\n        image = image.to(device)\n        with torch.no_grad():\n            out = classifier(image)\n            pred = out.argmax(dim=1).cpu().numpy()\n            \n        preds.append(pred)\n        labels.append(label.numpy())\n        \n    preds = np.concatenate(preds)\n    labels = np.concatenate(labels)\n    accuracy = sklearn.metrics.accuracy_score(y_pred=preds, y_true=labels)\n    metric['valid/accuracy'] = accuracy\n    \n    print(f\"============== Valid accuracy on {epoch}: {metric['valid/accuracy']} ==============\")\n    \n    log.append(metric)\n    \n    if accuracy > best_score:\n        best_score = accuracy\n        torch.save(classifier.state_dict(), 'best.pth')\n    torch.save(classifier.state_dict(), 'model.pth')\n    \n    with open('log.json', 'w') as f_out:\n        json.dump(log, f_out, indent=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}