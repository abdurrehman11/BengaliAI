{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport sys\nsys.path.append('../input/efficientnet-pytorch/EfficientNet-PyTorch/EfficientNet-PyTorch-master')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import glob\nimport math\nimport time\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport PIL.Image\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tqdm.notebook import tqdm\nimport sklearn.metrics\nfrom sklearn.model_selection import StratifiedKFold\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data.sampler import RandomSampler\nfrom torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\nimport albumentations\nfrom efficientnet_pytorch import model as enet\n\ndevice = torch.device('cuda')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kernel_type = 'effnet-b1-seen'\nenet_type = 'efficientnet-b1'\nfold = 0\nHEIGHT = 137\nWIDTH = 236\nimage_size = 128\ncut_size = int(image_size * 0.85)\ndata_dir = '../input/bengaliaicv19feather'\nbatch_size = 128\nnum_workers = 4\ninit_lr = 0.001\nc0_dim = 1295   # total unique graphemes\nc1_dim = 168    # total grapheme_root labels\nc2_dim = 11    # total grapheme_vowel labels\nc3_dim = 7    # total grapheme_consonant labels\nout_dim = c0_dim + c1_dim + c2_dim + c3_dim\nloss_weight = [4, 2, 1, 1, 2]\nn_epochs = 30\nuse_amp = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"files_train = [f'train_image_data_{fid}.feather' for fid in range(4)]\nfiles_test = [f'test_image_data_{fid}.feather' for fid in range(4)]\n\nprint(files_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(\"../input/bengaliai-cv19/train.csv\")\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_class_map = pd.read_csv(\"../input/bengaliai-cv19/class_map.csv\")\ndf_class_map.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"id2grapheme = {i: grapheme for i, grapheme in enumerate(df_train.grapheme.unique())}\ngrapheme2id = {grapheme: i for i, grapheme in enumerate(df_train.grapheme.unique())}\n\ndf_train['grapheme_id'] = df_train.grapheme.map(grapheme2id)\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_fold = 5\nskf = StratifiedKFold(n_fold, random_state=42, shuffle=True)\n\nfor i_fold, (train_idx, valid_idx) in enumerate(skf.split(df_train, df_train.grapheme)):\n    df_train.loc[valid_idx, 'kfold'] = i_fold\n    \ndf_train['kfold'] = df_train['kfold'].astype(int)\n\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_label_map = []\n\nfor i, df in tqdm(df_train.groupby('grapheme_id')):\n    df_label_map.append(df.iloc[:, 1:6].drop_duplicates())\n    \ndf_label_map = pd.concat(df_label_map).reset_index(drop=True)\n\ndf_label_map.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_data(files):\n    tmp = []\n    for f in files:\n        F = os.path.join(data_dir, f)\n        data = pd.read_feather(F)\n        res = data.iloc[:, 1:].values\n        imgs = []\n        for i in tqdm(range(res.shape[0])):\n            img = res[i].squeeze().reshape(HEIGHT, WIDTH)\n            img = cv2.resize(img, (128, 128))\n            imgs.append(img)\n        imgs = np.asarray(imgs)\n        \n        tmp.append(imgs)\n    tmp = np.concatenate(tmp, axis=0)\n    \n    return tmp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BengaliDataset(Dataset):\n    def __init__(self, img_csv, img_arr, img_idx, mode, transform=None):\n        self.img_csv = img_csv\n        self.img_arr = img_arr\n        self.img_idx = img_idx\n        self.mode = mode\n        self.transform = transform\n        \n    def __len__(self):\n        return self.img_idx.shape[0]\n    \n    def __getitem__(self, index):\n        index = self.img_idx[index]\n        this_image_id = self.img_csv.iloc[index].image_id\n        \n        image = self.img_arr[index]\n        image = 255 - image\n        \n        if self.transform is not None:\n            image_orign = image.astype(np.float32).copy()\n            res = self.transform(image=image)    # apply augmentations\n            image = res['image'].astype(np.float32)\n        else:\n            image_orign = image.astype(np.float32).copy()\n            image = image.astype(np.float32)\n            \n        image_orign /= 255\n        # make 1-channel to 3-channel image (Channels, Height, Width)\n        image_orign = image_orign[np.newaxis, :, :]    # (1, 128, 128)\n        image_orign = np.repeat(image_orign, 3, 0)     # (3, 128, 128)\n            \n        image /= 255\n        # make 1-channel to 3-channel image (Channels, Height, Width)\n        image = image[np.newaxis, :, :]    # (1, 128, 128)\n        image = np.repeat(image, 3, 0)     # (3, 128, 128)\n        \n        if self.mode == 'test':\n            return torch.tensor(image)\n        else:\n            label_0 = self.img_csv.iloc[index].grapheme_id\n            label_1 = self.img_csv.iloc[index].grapheme_root\n            label_2 = self.img_csv.iloc[index].vowel_diacritic\n            label_3 = self.img_csv.iloc[index].consonant_diacritic\n            label = [label_0, label_1, label_2, label_3]\n            \n            return torch.tensor(image), torch.tensor(image_orign), torch.tensor(label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = read_data(files_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transforms_train = albumentations.Compose([\n    # Cutout --> CoarseDropout of the square regions in the image\n    albumentations.Cutout(num_holes=1, max_h_size=cut_size, max_w_size=cut_size, p=0.7),\n])\n\ntransforms_valid = albumentations.Compose([\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_show = df_train.iloc[:1000]\ndataset_show = BengaliDataset(df_show, data, list(range(df_show.shape[0])), 'train', transform=transforms_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pylab import rcParams\n\nrcParams['figure.figsize'] = 20, 10\nfor i in range(2):\n    f, axarr = plt.subplots(1, 5)\n    for p in range(5):\n        idx = np.random.randint(0, df_show.shape[0])\n        img, img_org, label = dataset_show[idx]\n        # convert from (C, H, W) to (H, W, C) --> (3, 128, 128) to (128, 128, 3)\n        axarr[p].imshow(img.transpose(0, 1).transpose(1, 2).squeeze())\n        axarr[p].set_title(idx)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"pretrained_enet_dict = {f'efficientnet-b{i}': f \n                        for i, f in enumerate(sorted(glob.glob('../input/efficientnet-pytorch/*pth')))}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pretrained_enet_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DenseCrossEntropy(nn.Module):\n    def forward(self, x, target, reduction='mean'):\n        x = x.float()\n        target = target.float()\n        logprobs = torch.nn.functional.log_softmax(x, dim=-1)\n        \n        loss = -1 * (target * logprobs)\n        loss = loss.sum(-1)\n        \n        if reduction == 'mean':\n            return loss.mean()\n        elif reduction == 'sum':\n            return loss.sum()\n        elif reduction == 'none':\n            return loss\n\n\nclass ArcFaceLoss(nn.modules.Module):\n    def __init__(self, s=30.0, m=0.5, reduction='mean'):\n        super().__init__()\n        self.reduction = reduction\n        self.s = s\n        self.cos_m = math.cos(m)             #  0.87758\n        self.sin_m = math.sin(m)             #  0.47943\n        self.th = math.cos(math.pi - m)      # -0.87758\n        self.mm = math.sin(math.pi - m) * m  #  0.23971\n        \n    def forward(self, logits, labels):\n        logits = logits.float()  # float16 to float32 (if used float16)\n        cosine = logits\n        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n        \n        output = (labels * phi) + ((1.0 - labels) * cosine)\n        output *= self.s\n        \n        loss = DenseCrossEntropy()(output, labels, self.reduction)\n        \n        return loss / 2\n         \n\nsigmoid = torch.nn.Sigmoid()\n\n\nclass Swish(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, i):\n        result = i * sigmoid(i)\n        ctx.save_for_backward(i)\n        return result\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        i = ctx.saved_variables[0]\n        sigmoid_i = sigmoid(i)\n        return grad_output * (sigmoid_i * (1 + i * (1 - sigmoid_i)))\n\n\n# To apply our Function, we use Function.apply method.\nswish = Swish.apply\n\n\nclass Swish_Module(nn.Module):\n    def forward(self, x):\n        return swish(x)\n\nclass ArcMarginProduct(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1. / math.sqrt(self.weight.size(1))\n        self.weight.data.uniform_(-stdv, stdv)\n\n    def forward(self, features):\n        cosine = F.linear(F.normalize(features), F.normalize(self.weight))\n        return cosine\n\n\nclass enet_arcface_v2(nn.Module):\n    def __init__(self, backbone, out_dim_1, out_dim_2):\n        \"\"\"\n        :param backbone: name of efficient-net (b0, b1, ..., b7)\n        :param out_dim_1: no. unique graphemes (1295)\n        :param out_dim_2: total. no of grapheme components (root=168 + vowel=11 + consonant=7 == 186)\n        \"\"\"\n        super(enet_arcface_v2, self).__init__()\n        self.enet = enet.EfficientNet.from_name(backbone)\n        self.enet.load_state_dict(torch.load(pretrained_enet_dict[backbone]), strict=True)\n        self.dropouts = nn.ModuleList([\n            nn.Dropout(0.5) for _ in range(5)\n        ])\n\n        self.gfc = nn.Linear(self.enet._fc.in_features, 4096)\n        self.metric_classify = ArcMarginProduct(4096, out_dim_1)\n        self.myfc_1 = nn.Linear(4096, out_dim_1)\n        self.myfc_2_1 = nn.Linear(4096, 512)\n        self.myfc_2_2 = nn.Linear(512, out_dim_2)\n        self.enet._fc = nn.Identity()\n\n    def extract(self, x):\n        return self.enet(x)\n\n    def forward(self, x):\n        x = self.extract(x)\n        x = Swish_Module()(self.gfc(x))\n        for i, dropout in enumerate(self.dropouts):\n            if i == 0:\n                out_1 = self.myfc_1(dropout(x))\n                out_2 = self.myfc_2_1(dropout(x))\n            else:\n                out_1 += self.myfc_1(dropout(x))\n                out_2 += self.myfc_2_1(dropout(x))\n\n        out_1 /= len(self.dropouts)\n        out_2 /= len(self.dropouts)\n\n        out_2 = self.myfc_2_2(Swish_Module()(out_2))\n        metric_output = self.metric_classify(x)\n\n        return out_1, out_2, metric_output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- **ArcFace** -> https://medium.com/@peaceful0907/face-recognition-arcface-cf814fb5549e\n- **EfficientNet** -> https://github.com/lukemelas/EfficientNet-PyTorch\n- **Save and Load Pytorch Models** -> https://pytorch.org/tutorials/beginner/saving_loading_models.html\n- **Pytorch nn Module** -> https://pytorch.org/docs/stable/nn.html\n- **Multi-Sample Dropout** -> https://medium.com/@kushajreal/multi-sample-dropout-method-that-reduces-the-training-time-by-4-times-56b52edef0d7\n- **Pytorch Autograd** -> https://towardsdatascience.com/getting-started-with-pytorch-part-1-understanding-how-automatic-differentiation-works-5008282073ec\n- **AMP(Automatic Mixed Precision)** -> https://nvidia.github.io/apex/amp.html\n- **Cross Entropy** -> ttps://machinelearningmastery.com/cross-entropy-for-machine-learning/"},{"metadata":{},"cell_type":"markdown","source":"## Image Mix Techniques"},{"metadata":{"trusted":true},"cell_type":"code","source":"def rand_bbox(size, lam):\n    W = size[2]\n    H = size[3]\n    cut_rat = np.sqrt(1. - lam)\n    cut_w = np.int(W * cut_rat)\n    cut_h = np.int(H * cut_rat)\n\n    # uniform\n    cx = np.random.randint(W)\n    cy = np.random.randint(H)\n\n    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n    bby1 = np.clip(cy - cut_h // 2, 0, H)\n    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n    bby2 = np.clip(cy + cut_h // 2, 0, H)\n    return bbx1, bby1, bbx2, bby2\n\n\ndef mixup(data, target, alpha):\n    indices = torch.randperm(data.size(0))\n    shuffled_data = data[indices]\n    shuffled_target = target[indices]\n\n    lam = np.random.beta(alpha, alpha)\n    data = data * lam + shuffled_data * (1 - lam)\n    targets = (target, shuffled_target, lam)\n\n    return data, targets\n\n\ndef cutmix(data, target, alpha, clip=[0.3, 0.7]):\n    indices = torch.randperm(data.size(0))\n    shuffled_data = data[indices]\n    shuffled_target = target[indices]\n\n    lam = np.clip(np.random.beta(alpha, alpha), clip[0], clip[1])\n    bbx1, bby1, bbx2, bby2 = rand_bbox(data.size(), lam)\n    data[:, :, bbx1:bbx2, bby1:bby2] = data[indices, :, bbx1:bbx2, bby1:bby2]\n    # adjust lambda to exactly match pixel ratio\n    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (data.size()[-1] * data.size()[-2]))\n    targets = (target, shuffled_target, lam)\n\n    return data, targets","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loss Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def criterion(logits_1, logits_2, metric_logits, target, loss_weight=loss_weight, is_val=False):\n    # Given in the competition -> final score is the weighted average \n    # of those three scores, with the grapheme root given double weight.\n    \n    # loss_0 -> graphemes, loss_1 -> root, loss_2 -> vowel, loss_3 -> consonant\n    loss_1 = nn.CrossEntropyLoss()(logits_2[:, :c1_dim], target[:, 1]) * loss_weight[1]\n    loss_2 = nn.CrossEntropyLoss()(logits_2[:, c1_dim:c1_dim+c2_dim], target[:, 2]) * loss_weight[2]\n    loss_3 = nn.CrossEntropyLoss()(logits_2[:, c1_dim+c2_dim:], target[:, 3]) * loss_weight[3]\n    \n    if is_val:\n        loss = (loss_1 + loss_2 + loss_3) / sum(loss_weight[1:4])\n    else:\n        loss_metric = ArcFaceLoss()(metric_logits, F.one_hot(target[:, 0], c0_dim).float()) * loss_weight[4]\n        loss_0 = nn.CrossEntropyLoss()(logits_1, target[:, 0]) * loss_weight[0]\n        \n        loss = (loss_0 + loss_1 + loss_2 + loss_3 + loss_metric) / sum(loss_weight)\n        \n    return loss\n\n# In case of mixup and cutmix, we compare the predictions for with both\n# target(becuase in case of mixup and cutmix, each image will be a combination\n# of two images so we compare each image prediction with both of its targets)\ndef criterion_mix(logits_1, logits_2, metric_logits, target, loss_weight=loss_weight):\n    target, shuffled_target, lam = target\n    \n    loss_0 = nn.CrossEntropyLoss()(logits_1, target[:, 0]) * loss_weight[0]\n    loss_1 = nn.CrossEntropyLoss()(logits_2[:, :c1_dim], target[:, 1]) * loss_weight[1]\n    loss_2 = nn.CrossEntropyLoss()(logits_2[:, c1_dim:c1_dim+c2_dim], target[:, 2]) * loss_weight[2]\n    loss_3 = nn.CrossEntropyLoss()(logits_2[:, c1_dim+c2_dim:], target[:, 3]) * loss_weight[3]\n    loss_metric = ArcFaceLoss()(metric_logits, F.one_hot(target[:, 0], c0_dim).float()) * loss_weight[4]\n    \n    loss = (loss_0 + loss_1 + loss_2 + loss_3 + loss_metric) / sum(loss_weight)\n    \n    loss_0_mix = nn.CrossEntropyLoss()(logits_1, shuffled_target[:, 0]) * loss_weight[0]\n    loss_1_mix = nn.CrossEntropyLoss()(logits_2[:, :c1_dim], shuffled_target[:, 1]) * loss_weight[1]\n    loss_2_mix = nn.CrossEntropyLoss()(logits_2[:, c1_dim:c1_dim+c2_dim], shuffled_target[:, 2]) * loss_weight[2]\n    loss_3_mix = nn.CrossEntropyLoss()(logits_2[:, c1_dim+c2_dim:], shuffled_target[:, 3]) * loss_weight[3]\n    loss_metric_mix = ArcFaceLoss()(metric_logits, F.one_hot(shuffled_target[:, 0], c0_dim).float()) * loss_weight[4]\n    \n    loss_mix = (loss_0_mix + loss_1_mix + loss_2_mix + loss_3_mix + loss_metric_mix) / sum(loss_weight)\n    \n    return lam * loss + (1 - lam) * loss_mix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train & Val"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_score(solution, submission):\n\n    scores = []\n    for component in ['grapheme_root', 'consonant_diacritic', 'vowel_diacritic']:\n        y_true_subset = solution[component].values\n        y_pred_subset = submission[component].values\n        scores.append(sklearn.metrics.recall_score(\n            y_true_subset, y_pred_subset, average='macro'))\n    final_score = np.average(scores, weights=[2,1,1])\n    return final_score\n\n\ndef train_epoch(loader, model, optimizer):\n\n    model.train()\n    train_loss = []\n    \n    bar = tqdm(loader)\n    for (data, data_org, target) in bar:\n        data, data_org, target = data.to(device), data_org.to(device), target.to(device)\n        \n        # right now, I am not using mixup and cutmix to avoid\n        # complexity but they can be used with some percentage\n        # decided by experiments (e.g. 25% cutmix, 10% mixup)\n        rand_p = np.random.rand()\n        if rand_p <= 0.0:\n            data, target = mixup(data_org, target, 1.)\n            loss_func = criterion_mix\n        elif 0.0 < rand_p <= 0.0:\n            data, target = cutmix(data_org, target, 1.)\n            loss_func = criterion_mix\n        else:\n            loss_func = criterion\n        \n        # reset the gradients because by default \n        # they are accumulated in pytorch\n        optimizer.zero_grad()\n        \n        # forward pass \n        logits_1, logits_2, metric_logits = model(data)\n        \n        # calculate loss\n        loss = loss_func(logits_1, logits_2, metric_logits, target)\n        \n        # backward pass\n        if not use_amp:\n            loss.backward()\n        else:\n            with amp.scale_loss(loss, optimizer) as scaled_loss:\n                scaled_loss.backward()\n        \n        # update model params\n        optimizer.step()\n        \n        loss_np = loss.detach().cpu().numpy()\n        train_loss.append(loss_np)\n        \n        smooth_loss = sum(train_loss[-20:]) / min(len(train_loss), 20)\n        bar.set_description('loss: %.5f, smth: %.5f' % (loss_np, smooth_loss))\n        \n    return train_loss\n\n\ndef val_epoch(loader, model, get_output=False):\n    \n    model.eval()\n    val_loss = []\n    outputs = []\n    LOGITS_1, LOGITS_2, LOGITS_M = [], [], []\n    p1, p2, p3 = [], [], []\n    masks = []\n    acc, acc1, acc2, acc3 = 0.0, 0.0, 0.0, 0.0\n    \n    with torch.no_grad():\n        for (data, data_org, target) in tqdm(loader):\n            data, target = data.to(device), target.to(device)\n            \n            # forward pass\n            logits_1, logits_2, metric_logits = model(data)\n            \n            # calculate loss\n            loss = criterion(logits_1, logits_2, metric_logits, target, is_val=True)\n            \n            pred = logits_1.argmax(1).detach()\n            pred1 = logits_2[:, :c1_dim].argmax(1).detach()\n            pred2 = logits_2[:, c1_dim:].argmax(1).detach()\n            pred3 = logits_2[:, c1_dim+c2_dim:].argmax(1).detach()\n            \n            outputs.append(pred)\n            p1.append(pred1)\n            p2.append(pred2)\n            p3.append(pred3)\n            \n            acc += (target[:, 0] == pred).sum().cpu().numpy()\n            acc1 += (target[:, 1] == pred1).sum().cpu().numpy()\n            acc2 += (target[:, 2] == pred2).sum().cpu().numpy()\n            acc3 += (target[:, 3] == pred3).sum().cpu().numpy()\n            \n            if get_output:\n                LOGITS_1.append(logits_1)\n                LOGITS_2.append(logits_2)\n            LOGITS_M.append(metric_logits)\n            \n            val_loss.append(loss.detach().cpu().numpy())\n            \n        val_loss = np.mean(val_loss)\n        acc = acc / len(dataset_valid) * 100\n        acc1 = acc1 / len(dataset_valid) * 100\n        acc2 = acc2 / len(dataset_valid) * 100\n        acc3 = acc3 / len(dataset_valid) * 100\n        \n    # concatenate all outputs tensors\n    preds = torch.cat(outputs).cpu().numpy()\n    solution = df_train.iloc[valid_idx]  # targets\n    submission1 = df_label_map.iloc[preds]  # predictions\n    score1 = get_score(solution, submission1)\n    \n    submission2 = pd.DataFrame({\n        'grapheme_root': torch.cat(p1).cpu().numpy(),\n        'vowel_diacritic': torch.cat(p2).cpu().numpy(),\n        'consonant_diacritic': torch.cat(p3).cpu().numpy(),\n    })\n    \n    score2 = get_score(solution, submission2)\n    \n    # shape -> (no. of images, 1295)\n    LOGITS_M = torch.cat(LOGITS_M).cpu().numpy()\n    \n    max_p = LOGITS_M.max(axis=1)\n    \n    seen_class_acc = 1 - (targets0[np.where(max_p > 0.9)[0]] >= c0_dim).mean()\n    \n    try:\n        arcface_recall = np.where((max_p > 0.9) * (targets0 < c0_dim))[0].shape[0] / np.where(targets0 < c0_dim)[0].shape[0]\n    except:\n        arcface_recall = 0\n        \n    if get_output:\n        LOGITS_1 = torch.cat(LOGITS_1).cpu().numpy()\n        LOGITS_2 = torch.cat(LOGITS_2).cpu().numpy()\n        return LOGITS_1, LOGITS_2, LOGITS_M\n    else:\n        return val_loss, acc, acc1, acc2, acc3, score1, score2, seen_class_acc, arcface_recall","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Main Loop"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_fold = 5\n\nrecord = [{'train_loss': [], 'val_loss': [], 'score1': [], 'score2': []}\n          for i in range(n_fold)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i_fold = fold\ntrain_idx, valid_idx = np.where((df_train['kfold'] != i_fold))[0], np.where((df_train['kfold'] == i_fold))[0]\n\ndataset_train = BengaliDataset(df_train, data, train_idx, 'train', transform=transforms_train)\ndataset_valid = BengaliDataset(df_train, data, valid_idx, 'val', transform=transforms_valid)\n\ntrain_loader = DataLoader(dataset_train, batch_size=batch_size, \n                          sampler=RandomSampler(dataset_train),\n                          num_workers=num_workers)\nvalid_loader = DataLoader(dataset_valid, batch_size=batch_size, \n                          sampler=None,\n                          num_workers=num_workers)\n\ntargets0 = df_train.loc[valid_idx]['grapheme_id'].values\n\nmodel = enet_arcface_v2(enet_type, out_dim_1=c0_dim, out_dim_2=c1_dim+c2_dim+c3_dim)\nmodel.to(device)\n\nmax_score = 0\nmodel_file = f'{kernel_type}_best_fold{i_fold}.pth'\n\nprint('Training All Layers...')\noptimizer = optim.Adam(model.parameters(), lr=init_lr)\n\n# Allow Amp to perform casts as required by the opt_level (need to STUDY...)\nif use_amp:\n    model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\n\nlr_scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, n_epochs)\n\nfor epoch in range(1, n_epochs+1):\n    print(time.ctime(), 'Epoch:', epoch)\n    # to use the initial lr in the first epoch,\n    # use --> step(0)\n    lr_scheduler_cosine.step(epoch-1)\n    \n    # call the training function\n    train_loss = train_epoch(train_loader, model, optimizer)\n    \n    # call the validation function\n    val_loss, acc, acc1, acc2, acc3, score1, score2, seen_class_acc, arcface_recall = val_epoch(valid_loader, model)\n    \n    model_stats = (\n        f'Epoch {epoch}, lr: {optimizer.param_groups[0][\"lr\"]:.7f}\\n'\n        f'train loss: {np.mean(train_loss):.5f}\\n'\n        f'val loss: {np.mean(val_loss):.5f}\\n'\n        f'acc: {(acc):.5f}\\n'\n        f'acc1: {(acc1):.5f}\\n'\n        f'acc2: {(acc2):.5f}\\n'\n        f'acc3: {(acc3):.5f}\\n'\n        f'seen_class_acc: {(seen_class_acc):.5f}\\n'\n        f'arcface_recall: {(arcface_recall):.5f}\\n'\n        f'score1: {(score1):.6f}\\n'\n        f'score2: {(score2):.6f}\\n'\n    )\n    \n    content = time.ctime() + '\\n' + model_stats\n    print(content)\n    \n    with open(f'log_{kernel_type}.txt', 'a') as appender:\n        appender.write(content + '\\n')\n        \n    if score1 >= max_score:\n        print('score2 ({:.6f} --> {:.6f}).  Saving model ...'.format(max_score, score1))\n        torch.save(model.state_dict(), model_file)\n        max_score = score1\n        \n    record[i_fold]['train_loss'].append(np.mean(train_loss))\n    record[i_fold]['val_loss'].append(val_loss)\n    record[i_fold]['score1'].append(np.mean(score1))\n    record[i_fold]['score2'].append(score2)\n    \n# save the trained model\ntorch.save(model.state_dict(), os.path.join(f'{kernel_type}_model_fold_{i_fold}.pth'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}